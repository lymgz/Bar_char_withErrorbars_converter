#!/usr/bin/env python3
"""
å¸¦è¯¯å·®çº¿æŸ±çŠ¶å›¾æ•°æ®è½¬æ¢å·¥å…·
æ”¯æŒå¤šç§è¯¯å·®çº¿ç±»å‹è½¬æ¢ä¸ºMetaåˆ†ææ ‡å‡†æ ¼å¼
"""

import csv
import os
import sys
import json
import pandas as pd
import math
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from error_detector import ErrorBarDetector
from statistical_engine import StatisticalEngine

class BarChartConverter:
    def __init__(self):
        self.detector = ErrorBarDetector()
        self.engine = StatisticalEngine()
        self.template_filename = "template.csv"
        self.data_filename = "data.csv"
        
    def generate_template(self, indicators_count: int = 4) -> str:
        """ç”ŸæˆCSVæ¨¡æ¿æ–‡ä»¶"""
        template_content = []
        
        # è¡¨å¤´è¯´æ˜
        template_content.append(["# å¸¦è¯¯å·®çº¿æŸ±çŠ¶å›¾æ•°æ®è¾“å…¥æ¨¡æ¿"])
        template_content.append(["# è¯¯å·®çº¿ç±»å‹: SE(æ ‡å‡†è¯¯å·®), SD(æ ‡å‡†å·®), CI95(95%ç½®ä¿¡åŒºé—´), CI99(99%ç½®ä¿¡åŒºé—´), 2SE(2å€æ ‡å‡†è¯¯å·®), ASYMMETRIC(éå¯¹ç§°è¯¯å·®çº¿)"])
        template_content.append(["# éå¯¹ç§°è¯¯å·®çº¿æ ¼å¼: Error_Bar_Upper/Error_Bar_Lower (ä¾‹å¦‚: 1.5/0.8)"])
        template_content.append(["# è¯·åœ¨Error_Typeè¡Œä¸­æ˜ç¡®æ ‡æ³¨è¯¯å·®çº¿ç±»å‹"])
        template_content.append([""])
        
        # åŸºçº¿ç»„
        header = ["Baseline"] + [f"Indicator{i+1}" for i in range(indicators_count)]
        template_content.append(header)
        
        data_items = ["Mean", "Error_Bar", "Error_Type", "Sample_Size"]
        for item in data_items:
            row = [item] + [""] * indicators_count
            template_content.append(row)
        
        # ç©ºè¡Œåˆ†éš”
        template_content.append([""] * (indicators_count + 1))
        
        # å¹²é¢„ç»„
        header = ["Intervention"] + [f"Indicator{i+1}" for i in range(indicators_count)]
        template_content.append(header)
        
        for item in data_items:
            row = [item] + [""] * indicators_count
            template_content.append(row)
        
        # å†™å…¥æ–‡ä»¶
        with open(self.template_filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerows(template_content)
        
        return self.template_filename
    
    def read_csv_data(self, filename: str) -> Dict:
        """è¯»å–CSVæ•°æ®å¹¶è§£æ"""
        if not os.path.exists(filename):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {filename}")
        
        with open(filename, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            rows = list(reader)
        
        return self._parse_csv_structure(rows)
    
    def _parse_csv_structure(self, rows: List[List[str]]) -> Dict:
        """è§£æCSVç»“æ„ï¼Œæ”¯æŒåŠ¨æ€åˆ—æ•°"""
        groups = {}
        current_group = None
        current_data = {}
        
        for row in rows:
            if not row or all(cell.strip() == "" for cell in row):
                # ç©ºè¡Œï¼Œç»“æŸå½“å‰ç»„
                if current_group and current_data:
                    groups[current_group] = current_data
                    current_data = {}
                continue
            
            # è·³è¿‡æ³¨é‡Šè¡Œ
            if row[0].startswith('#'):
                continue
            
            first_cell = row[0].strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»„æ ‡é¢˜
            if first_cell in ["åŸºçº¿ç»„", "å¹²é¢„ç»„", "Baseline", "Intervention"] or first_cell.endswith("ç»„"):
                if current_group and current_data:
                    groups[current_group] = current_data
                
                current_group = first_cell
                current_data = {}
                
                # æ£€æµ‹æŒ‡æ ‡æ•°é‡
                indicators = [cell.strip() for cell in row[1:] if cell.strip()]
                current_data['indicators'] = indicators
                current_data['indicator_count'] = len(indicators)
                current_data['data'] = {}
                continue
            
            # æ•°æ®è¡Œ
            data_labels = ["Mean", "Error_Bar", "Error_Type", "Sample_Size", "å‡å€¼", "è¯¯å·®çº¿", "è¯¯å·®ç±»å‹", "æ ·æœ¬é‡"]
            
            if current_group and first_cell in data_labels:
                # æ ‡å‡†åŒ–æ ‡ç­¾ä¸ºè‹±æ–‡
                label_mapping = {
                    "å‡å€¼": "Mean",
                    "è¯¯å·®çº¿": "Error_Bar", 
                    "è¯¯å·®ç±»å‹": "Error_Type",
                    "æ ·æœ¬é‡": "Sample_Size"
                }
                normalized_label = label_mapping.get(first_cell, first_cell)
                
                values = []
                for cell in row[1:]:
                    cell_value = cell.strip()
                    if cell_value:
                        if normalized_label in ["Mean", "Sample_Size"]:
                            try:
                                values.append(float(cell_value))
                            except ValueError:
                                values.append(None)
                        elif normalized_label == "Error_Bar":
                            # æ”¯æŒéå¯¹ç§°è¯¯å·®çº¿æ ¼å¼ "upper/lower"
                            if '/' in cell_value:
                                values.append(cell_value)  # ä¿æŒå­—ç¬¦ä¸²æ ¼å¼
                            else:
                                try:
                                    values.append(float(cell_value))
                                except ValueError:
                                    values.append(None)
                        else:  # Error_Type
                            values.append(cell_value.upper())
                    else:
                        values.append(None)
                
                current_data['data'][normalized_label] = values
        
        # å¤„ç†æœ€åä¸€ç»„
        if current_group and current_data:
            groups[current_group] = current_data
        
        return groups
    
    def analyze_error_types(self, groups: Dict) -> Dict:
        """åˆ†æè¯¯å·®çº¿ç±»å‹å’Œæ•°æ®è´¨é‡"""
        analysis = {}
        
        for group_name, group_data in groups.items():
            indicators_analysis = []
            indicator_count = group_data.get('indicator_count', 0)
            
            for i in range(indicator_count):
                indicator_data = self._extract_indicator_data(group_data, i)
                error_type = indicator_data.get('Error_Type', 'UNKNOWN')
                
                # æ£€æµ‹å’ŒéªŒè¯è¯¯å·®çº¿ç±»å‹
                detected_type, confidence = self.detector.detect_error_type(
                    indicator_data.get('Mean'),
                    indicator_data.get('Error_Bar'),
                    error_type,
                    indicator_data.get('Sample_Size')
                )
                
                indicators_analysis.append({
                    'indicator_index': i + 1,
                    'indicator_name': group_data['indicators'][i] if i < len(group_data['indicators']) else f"æŒ‡æ ‡{i+1}",
                    'declared_type': error_type,
                    'detected_type': detected_type,
                    'confidence': confidence,
                    'data_complete': self._check_data_completeness(indicator_data),
                    'data': indicator_data
                })
            
            analysis[group_name] = {
                'indicators': indicators_analysis,
                'indicator_count': indicator_count,
                'overall_quality': self._assess_group_quality(indicators_analysis)
            }
        
        return analysis
    
    def _extract_indicator_data(self, group_data: Dict, indicator_index: int) -> Dict:
        """æå–ç‰¹å®šæŒ‡æ ‡çš„æ•°æ®"""
        indicator_data = {}
        
        for data_type, values in group_data['data'].items():
            if indicator_index < len(values) and values[indicator_index] is not None:
                indicator_data[data_type] = values[indicator_index]
        
        # å¤„ç†éå¯¹ç§°è¯¯å·®çº¿æ ¼å¼
        if 'Error_Type' in indicator_data and indicator_data['Error_Type'] == 'ASYMMETRIC':
            if 'Error_Bar' in indicator_data:
                error_bar_value = indicator_data['Error_Bar']
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ "upper/lower"ï¼Œè½¬æ¢ä¸ºå¹³å‡å€¼
                if isinstance(error_bar_value, str):
                    try:
                        parts = error_bar_value.split('/')
                        if len(parts) == 2:
                            upper = float(parts[0])
                            lower = float(parts[1])
                            # ä½¿ç”¨å¹³å‡å€¼ä½œä¸ºè¯¯å·®çº¿å€¼
                            indicator_data['Error_Bar'] = (upper + lower) / 2
                            # ä¿å­˜åŸå§‹éå¯¹ç§°å€¼ç”¨äºå‚è€ƒ
                            indicator_data['Asymmetric_Error_Upper'] = upper
                            indicator_data['Asymmetric_Error_Lower'] = lower
                    except (ValueError, AttributeError):
                        pass
        
        return indicator_data
    
    def _check_data_completeness(self, indicator_data: Dict) -> bool:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        required_fields = ['Mean', 'Error_Bar', 'Error_Type', 'Sample_Size']
        return all(field in indicator_data and indicator_data[field] is not None for field in required_fields)
    
    def _assess_group_quality(self, indicators_analysis: List[Dict]) -> str:
        """è¯„ä¼°ç»„æ•°æ®è´¨é‡"""
        if not indicators_analysis:
            return "æ— æ•°æ®"
        
        complete_count = sum(1 for ind in indicators_analysis if ind['data_complete'])
        total_count = len(indicators_analysis)
        
        if complete_count == total_count:
            return "å®Œæ•´"
        elif complete_count >= total_count * 0.8:
            return "è‰¯å¥½"
        elif complete_count >= total_count * 0.5:
            return "ä¸€èˆ¬"
        else:
            return "ä¸å®Œæ•´"
    
    def convert_bar_data(self, filename: str, verbose: bool = False) -> Dict:
        """è½¬æ¢æŸ±çŠ¶å›¾æ•°æ®"""
        # è¯»å–æ•°æ®
        groups = self.read_csv_data(filename)
        
        # åˆ†æè¯¯å·®çº¿ç±»å‹
        analysis = self.analyze_error_types(groups)
        
        # æ‰§è¡Œè½¬æ¢
        results = {}
        
        for group_name, group_analysis in analysis.items():
            group_results = []
            
            if verbose:
                print(f"\n=== {group_name} åˆ†æ ===")
                print(f"æ£€æµ‹åˆ° {group_analysis['indicator_count']} ä¸ªæŒ‡æ ‡")
                print(f"æ•°æ®è´¨é‡: {group_analysis['overall_quality']}")
            
            for indicator in group_analysis['indicators']:
                if not indicator['data_complete']:
                    if verbose:
                        print(f"è·³è¿‡ {indicator['indicator_name']}: æ•°æ®ä¸å®Œæ•´")
                    continue
                
                indicator_data = indicator['data']
                
                # æ‰§è¡Œç»Ÿè®¡è½¬æ¢
                try:
                    result = self.engine.convert_to_mean_sd(
                        mean=indicator_data['Mean'],
                        error_bar=indicator_data['Error_Bar'],
                        error_type=indicator['detected_type'],
                        sample_size=int(indicator_data['Sample_Size'])
                    )
                    
                    result.update({
                        'indicator_name': indicator['indicator_name'],
                        'declared_type': indicator['declared_type'],
                        'detected_type': indicator['detected_type'],
                        'confidence': indicator['confidence'],
                        'conversion_method': result.get('method_used', 'unknown')
                    })
                    
                    group_results.append(result)
                    
                    if verbose:
                        print(f"{indicator['indicator_name']}: Mean={result['mean']:.3f}, SD={result['sd']:.3f} "
                              f"(ç±»å‹: {indicator['detected_type']}, ç½®ä¿¡åº¦: {indicator['confidence']:.2f})")
                
                except Exception as e:
                    if verbose:
                        print(f"è½¬æ¢å¤±è´¥ {indicator['indicator_name']}: {e}")
            
            results[group_name] = group_results
        
        return {
            'results': results,
            'analysis': analysis,
            'summary': self._generate_summary(analysis, results)
        }
    
    def _generate_summary(self, analysis: Dict, results: Dict) -> Dict:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        total_indicators = sum(group['indicator_count'] for group in analysis.values())
        successful_conversions = sum(len(group_results) for group_results in results.values())
        
        # ç»Ÿè®¡è¯¯å·®çº¿ç±»å‹åˆ†å¸ƒ
        type_distribution = {}
        for group_analysis in analysis.values():
            for indicator in group_analysis['indicators']:
                detected_type = indicator['detected_type']
                type_distribution[detected_type] = type_distribution.get(detected_type, 0) + 1
        
        return {
            'total_groups': len(analysis),
            'total_indicators': total_indicators,
            'successful_conversions': successful_conversions,
            'conversion_rate': successful_conversions / total_indicators if total_indicators > 0 else 0,
            'error_type_distribution': type_distribution,
            'recommendations': self._generate_recommendations(analysis)
        }
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        for group_name, group_analysis in analysis.items():
            incomplete_indicators = [
                ind['indicator_name'] for ind in group_analysis['indicators'] 
                if not ind['data_complete']
            ]
            
            if incomplete_indicators:
                recommendations.append(
                    f"{group_name}: è¡¥å……{', '.join(incomplete_indicators)}çš„å®Œæ•´æ•°æ®"
                )
            
            # æ£€æŸ¥è¯¯å·®çº¿ç±»å‹ä¸€è‡´æ€§
            declared_types = set(ind['declared_type'] for ind in group_analysis['indicators'] 
                                if ind['declared_type'] != 'UNKNOWN')
            if len(declared_types) > 1:
                recommendations.append(
                    f"{group_name}: å»ºè®®ç»Ÿä¸€è¯¯å·®çº¿ç±»å‹ï¼Œå½“å‰æ··åˆä½¿ç”¨äº†{', '.join(declared_types)}"
                )
        
        return recommendations
    
    def perform_group_comparisons(self, result_data: Dict, comparison_type: str = "intervention-baseline", 
                                confidence_level: float = 0.95) -> Dict:
        """æ‰§è¡Œç»„é—´æ¯”è¾ƒåˆ†æ"""
        results = result_data['results']
        comparisons = []
        
        if comparison_type in ["all", "intervention-baseline"]:
            # å¹²é¢„ç»„ vs åŸºçº¿ç»„æ¯”è¾ƒ
            baseline_results = results.get('Baseline', results.get('åŸºçº¿ç»„', []))
            intervention_results = results.get('Intervention', results.get('å¹²é¢„ç»„', []))
            
            # æŒ‰æŒ‡æ ‡é…å¯¹æ¯”è¾ƒ
            for i, baseline in enumerate(baseline_results):
                if i < len(intervention_results):
                    intervention = intervention_results[i]
                    comparison = self._calculate_group_comparison(
                        intervention, baseline, confidence_level
                    )
                    comparison.update({
                        'comparison_id': f"Intervention_vs_Baseline_{baseline['indicator_name']}",
                        'group1_name': 'Intervention',
                        'group2_name': 'Baseline',
                        'indicator_name': baseline['indicator_name'],
                        'group1_data': intervention,
                        'group2_data': baseline
                    })
                    comparisons.append(comparison)
        
        return {
            'comparisons': comparisons,
            'comparison_type': comparison_type,
            'confidence_level': confidence_level,
            'total_comparisons': len(comparisons),
            'significant_comparisons': sum(1 for c in comparisons if c['significant'])
        }
    
    def _calculate_group_comparison(self, group1_data: Dict, group2_data: Dict, 
                                  confidence_level: float = 0.95) -> Dict:
        """è®¡ç®—ä¸¤ç»„é—´çš„å·®å¼‚åˆ†æ"""
        delta_mean = group1_data['mean'] - group2_data['mean']
        
        # è®¡ç®—å·®å¼‚çš„æ ‡å‡†å·®
        sd_diff = math.sqrt(
            (group1_data['sd']**2 / group1_data['sample_size']) + 
            (group2_data['sd']**2 / group2_data['sample_size'])
        )
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        z_score = self._get_z_score(confidence_level)
        ci_lower = delta_mean - z_score * sd_diff
        ci_upper = delta_mean + z_score * sd_diff
        
        # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
        pooled_sd = math.sqrt(
            ((group1_data['sample_size'] - 1) * group1_data['sd']**2 + 
             (group2_data['sample_size'] - 1) * group2_data['sd']**2) / 
            (group1_data['sample_size'] + group2_data['sample_size'] - 2)
        )
        cohens_d = delta_mean / pooled_sd if pooled_sd > 0 else 0
        
        # Hedges' g (åå·®æ ¡æ­£çš„Cohen's d)
        correction_factor = 1 - (3 / (4 * (group1_data['sample_size'] + group2_data['sample_size']) - 9))
        hedges_g = cohens_d * correction_factor
        
        # è®¡ç®—på€¼ (åŒå°¾tæ£€éªŒ)
        df = group1_data['sample_size'] + group2_data['sample_size'] - 2
        t_stat = delta_mean / sd_diff if sd_diff > 0 else 0
        p_value = self._calculate_p_value(abs(t_stat), df)
        
        # åˆ¤æ–­æ˜¾è‘—æ€§
        alpha = 1 - confidence_level
        significant = p_value < alpha
        
        return {
            'delta_mean': round(delta_mean, 4),
            'sd_diff': round(sd_diff, 4),
            'ci_lower': round(ci_lower, 4),
            'ci_upper': round(ci_upper, 4),
            'confidence_level': confidence_level,
            'cohens_d': round(cohens_d, 4),
            'hedges_g': round(hedges_g, 4),
            'p_value': round(p_value, 4),
            'significant': significant,
            't_statistic': round(t_stat, 4),
            'degrees_of_freedom': df,
            'interpretation': self._interpret_comparison(delta_mean, ci_lower, ci_upper, significant)
        }
    
    def _get_z_score(self, confidence_level: float) -> float:
        """è·å–å¯¹åº”ç½®ä¿¡æ°´å¹³çš„zåˆ†æ•°"""
        z_scores = {
            0.90: 1.645,
            0.95: 1.96,
            0.99: 2.576
        }
        return z_scores.get(confidence_level, 1.96)
    
    def _calculate_p_value(self, t_stat: float, df: int) -> float:
        """ç®€åŒ–çš„på€¼è®¡ç®—ï¼ˆåŒå°¾æ£€éªŒï¼‰"""
        if df <= 0:
            return 1.0
        
        # ä½¿ç”¨è¿‘ä¼¼å…¬å¼
        if t_stat == 0:
            return 1.0
        elif t_stat > 4:
            return 0.0001
        elif t_stat > 3:
            return 0.01
        elif t_stat > 2:
            return 0.05
        elif t_stat > 1.5:
            return 0.1
        else:
            return 0.2
    
    def _interpret_comparison(self, delta_mean: float, ci_lower: float, 
                            ci_upper: float, significant: bool) -> str:
        """è§£é‡Šæ¯”è¾ƒç»“æœ"""
        if significant:
            if ci_lower > 0:
                return "æ˜¾è‘—å·®å¼‚ï¼šç»„1æ˜¾è‘—é«˜äºç»„2"
            elif ci_upper < 0:
                return "æ˜¾è‘—å·®å¼‚ï¼šç»„1æ˜¾è‘—ä½äºç»„2"
            else:
                return "æ˜¾è‘—å·®å¼‚ï¼šä½†ç½®ä¿¡åŒºé—´åŒ…å«0"
        else:
            return "æ— æ˜¾è‘—å·®å¼‚"
    
    def generate_meta_analysis_formats(self, result_data: Dict, comparison_data: Dict = None, 
                                     output_dir: str = "results") -> Dict[str, str]:
        """ç”Ÿæˆå¤šç§Metaåˆ†ææ ‡å‡†æ ¼å¼"""
        self.ensure_results_dir(output_dir)
        saved_files = {}
        
        # é€šç”¨Metaåˆ†ææ ¼å¼
        universal_data = self._create_universal_meta_format(result_data, comparison_data)
        universal_path = os.path.join(output_dir, "meta_universal.csv")
        universal_path = self.get_available_filename(universal_path)
        
        df_universal = pd.DataFrame(universal_data)
        df_universal.to_csv(universal_path, index=False, encoding='utf-8')
        saved_files['universal'] = universal_path
        
        # RevManæ ¼å¼
        revman_data = self._create_revman_format(result_data)
        revman_path = os.path.join(output_dir, "meta_revman.csv")
        revman_path = self.get_available_filename(revman_path)
        
        df_revman = pd.DataFrame(revman_data)
        df_revman.to_csv(revman_path, index=False, encoding='utf-8')
        saved_files['revman'] = revman_path
        
        # R MetaåŒ…æ ¼å¼
        if comparison_data:
            r_meta_data = self._create_r_meta_format(comparison_data)
            r_path = os.path.join(output_dir, "meta_r.csv")
            r_path = self.get_available_filename(r_path)
            
            df_r = pd.DataFrame(r_meta_data)
            df_r.to_csv(r_path, index=False, encoding='utf-8')
            saved_files['r_meta'] = r_path
        
        return saved_files
    
    def _create_universal_meta_format(self, result_data: Dict, comparison_data: Dict = None) -> List[Dict]:
        """åˆ›å»ºé€šç”¨Metaåˆ†ææ ¼å¼"""
        universal_data = []
        
        if comparison_data and comparison_data['comparisons']:
            for comparison in comparison_data['comparisons']:
                if 'Intervention_vs_Baseline' in comparison['comparison_id']:
                    universal_data.append({
                        'Study_ID': comparison['indicator_name'],
                        'Comparison_Type': 'Intervention-Baseline',
                        'Intervention_Mean': comparison['group1_data']['mean'],
                        'Intervention_SD': comparison['group1_data']['sd'],
                        'Intervention_N': comparison['group1_data']['sample_size'],
                        'Control_Mean': comparison['group2_data']['mean'],
                        'Control_SD': comparison['group2_data']['sd'],
                        'Control_N': comparison['group2_data']['sample_size'],
                        'Mean_Difference': comparison['delta_mean'],
                        'SD_Difference': comparison['sd_diff'],
                        'Effect_Size_Cohens_d': comparison['cohens_d'],
                        'Effect_Size_Hedges_g': comparison['hedges_g'],
                        'SE_Mean_Diff': comparison['sd_diff'],
                        '95_CI_Lower': comparison['ci_lower'],
                        '95_CI_Upper': comparison['ci_upper'],
                        'P_Value': comparison['p_value'],
                        'Significant': 'Yes' if comparison['significant'] else 'No',
                        'Error_Bar_Type': comparison['group1_data']['detected_type'],
                        'Conversion_Method': comparison['group1_data']['conversion_method'],
                        'Notes': comparison['interpretation']
                    })
        else:
            # å¦‚æœæ²¡æœ‰æ¯”è¾ƒæ•°æ®ï¼Œåªè¾“å‡ºåŸºç¡€è½¬æ¢ç»“æœ
            for group_name, group_results in result_data['results'].items():
                for result in group_results:
                    universal_data.append({
                        'Study_ID': result['indicator_name'],
                        'Group': group_name,
                        'Mean': result['mean'],
                        'SD': result['sd'],
                        'Sample_Size': result['sample_size'],
                        'Error_Bar_Type': result['detected_type'],
                        'Conversion_Method': result['conversion_method'],
                        'Confidence': result['confidence']
                    })
        
        return universal_data
    
    def _create_revman_format(self, result_data: Dict) -> List[Dict]:
        """åˆ›å»ºRevManæ ¼å¼"""
        revman_data = []
        results = result_data['results']
        
        baseline_results = results.get('Baseline', results.get('åŸºçº¿ç»„', []))
        intervention_results = results.get('Intervention', results.get('å¹²é¢„ç»„', []))
        
        for i, baseline in enumerate(baseline_results):
            if i < len(intervention_results):
                intervention = intervention_results[i]
                revman_data.append({
                    'Study_ID': baseline['indicator_name'],
                    'Intervention_Mean': intervention['mean'],
                    'Intervention_SD': intervention['sd'],
                    'Intervention_N': intervention['sample_size'],
                    'Control_Mean': baseline['mean'],
                    'Control_SD': baseline['sd'],
                    'Control_N': baseline['sample_size']
                })
        
        return revman_data
    
    def _create_r_meta_format(self, comparison_data: Dict) -> List[Dict]:
        """åˆ›å»ºR MetaåŒ…æ ¼å¼"""
        r_data = []
        
        for comparison in comparison_data['comparisons']:
            if 'Intervention_vs_Baseline' in comparison['comparison_id']:
                r_data.append({
                    'Study': comparison['indicator_name'],
                    'TE': comparison['delta_mean'],
                    'seTE': comparison['sd_diff'],
                    'n.e': comparison['group1_data']['sample_size'],
                    'n.c': comparison['group2_data']['sample_size'],
                    'mean.e': comparison['group1_data']['mean'],
                    'sd.e': comparison['group1_data']['sd'],
                    'mean.c': comparison['group2_data']['mean'],
                    'sd.c': comparison['group2_data']['sd']
                })
        
        return r_data
    
    def is_file_locked(self, filepath: str) -> bool:
        """æ£€æµ‹æ–‡ä»¶æ˜¯å¦è¢«å ç”¨"""
        if not os.path.exists(filepath):
            return False
        
        try:
            with open(filepath, 'a'):
                return False
        except (IOError, OSError):
            return True
    
    def get_available_filename(self, base_filename: str) -> str:
        """è·å–å¯ç”¨çš„æ–‡ä»¶åï¼Œå¤„ç†æ–‡ä»¶å ç”¨æƒ…å†µ"""
        name, ext = os.path.splitext(base_filename)
        
        if not self.is_file_locked(base_filename):
            return base_filename
        
        for i in range(1, 100):
            suffix = f"_{i:02d}"
            new_filename = f"{name}{suffix}{ext}"
            if not self.is_file_locked(new_filename):
                return new_filename
        
        return f"{name}_01{ext}"
    
    def ensure_results_dir(self, output_dir: str = "results") -> str:
        """ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        return output_dir
    
    def save_to_excel(self, result_data: Dict, comparison_data: Dict = None, 
                     output_dir: str = "results", base_filename: str = "bar_results.xlsx") -> str:
        """ä¿å­˜ç»“æœåˆ°Excelæ–‡ä»¶"""
        self.ensure_results_dir(output_dir)
        filepath = os.path.join(output_dir, base_filename)
        final_filepath = self.get_available_filename(filepath)
        
        with pd.ExcelWriter(final_filepath, engine='openpyxl') as writer:
            # Sheet1: è½¬æ¢ç»“æœ
            results_data = []
            for group_name, group_results in result_data['results'].items():
                for result in group_results:
                    results_data.append({
                        'Group': group_name,
                        'Indicator': result['indicator_name'],
                        'Mean': round(result['mean'], 4),
                        'SD': round(result['sd'], 4),
                        'Sample_Size': result['sample_size'],
                        'Error_Bar_Type': result['detected_type'],
                        'Declared_Type': result['declared_type'],
                        'Confidence': round(result['confidence'], 3),
                        'Conversion_Method': result['conversion_method']
                    })
            
            if results_data:
                df_results = pd.DataFrame(results_data)
                df_results.to_excel(writer, sheet_name='è½¬æ¢ç»“æœ', index=False)
            
            # Sheet2: ç»„é—´æ¯”è¾ƒç»“æœ (å¦‚æœæœ‰æ¯”è¾ƒæ•°æ®)
            if comparison_data and comparison_data['comparisons']:
                comparison_results = []
                for comp in comparison_data['comparisons']:
                    comparison_results.append({
                        'Comparison': f"{comp['group1_name']} vs {comp['group2_name']}",
                        'Indicator': comp['indicator_name'],
                        'Î”Mean': comp['delta_mean'],
                        'SD_diff': comp['sd_diff'],
                        '95%_CI_Lower': comp['ci_lower'],
                        '95%_CI_Upper': comp['ci_upper'],
                        'Cohens_d': comp['cohens_d'],
                        'Hedges_g': comp['hedges_g'],
                        'P_Value': comp['p_value'],
                        'Significant': 'Yes' if comp['significant'] else 'No',
                        'Interpretation': comp['interpretation']
                    })
                
                df_comparisons = pd.DataFrame(comparison_results)
                df_comparisons.to_excel(writer, sheet_name='ç»„é—´æ¯”è¾ƒç»“æœ', index=False)
            
            # Sheet3: æ•°æ®è´¨é‡åˆ†æ
            quality_data = []
            for group_name, group_analysis in result_data['analysis'].items():
                quality_data.append({
                    'Group': group_name,
                    'Total_Indicators': group_analysis['indicator_count'],
                    'Overall_Quality': group_analysis['overall_quality']
                })
            
            if quality_data:
                df_quality = pd.DataFrame(quality_data)
                df_quality.to_excel(writer, sheet_name='æ•°æ®è´¨é‡åˆ†æ', index=False)
            
            # Sheet4: æ‘˜è¦ä¿¡æ¯
            summary = result_data['summary']
            summary_data = [
                {'é¡¹ç›®': 'æ€»ç»„æ•°', 'å€¼': summary['total_groups']},
                {'é¡¹ç›®': 'æ€»æŒ‡æ ‡æ•°', 'å€¼': summary['total_indicators']},
                {'é¡¹ç›®': 'æˆåŠŸè½¬æ¢æ•°', 'å€¼': summary['successful_conversions']},
                {'é¡¹ç›®': 'è½¬æ¢æˆåŠŸç‡', 'å€¼': f"{summary['conversion_rate']*100:.1f}%"},
                {'é¡¹ç›®': 'å¤„ç†æ—¶é—´', 'å€¼': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            ]
            
            # æ·»åŠ è¯¯å·®çº¿ç±»å‹åˆ†å¸ƒ
            for error_type, count in summary['error_type_distribution'].items():
                summary_data.append({'é¡¹ç›®': f'è¯¯å·®çº¿ç±»å‹_{error_type}', 'å€¼': count})
            
            # å¦‚æœæœ‰æ¯”è¾ƒæ•°æ®ï¼Œæ·»åŠ æ¯”è¾ƒæ‘˜è¦
            if comparison_data:
                summary_data.extend([
                    {'é¡¹ç›®': 'æ€»æ¯”è¾ƒæ•°', 'å€¼': comparison_data['total_comparisons']},
                    {'é¡¹ç›®': 'æ˜¾è‘—æ¯”è¾ƒæ•°', 'å€¼': comparison_data['significant_comparisons']},
                    {'é¡¹ç›®': 'æ¯”è¾ƒç±»å‹', 'å€¼': comparison_data['comparison_type']},
                    {'é¡¹ç›®': 'ç½®ä¿¡æ°´å¹³', 'å€¼': f"{comparison_data['confidence_level']*100}%"}
                ])
            
            df_summary = pd.DataFrame(summary_data)
            df_summary.to_excel(writer, sheet_name='æ‘˜è¦ä¿¡æ¯', index=False)
            
            # å¦‚æœæœ‰å»ºè®®ï¼Œæ·»åŠ åˆ°æ”¹è¿›å»ºè®®å·¥ä½œè¡¨
            if result_data['summary']['recommendations']:
                rec_data = [{'å»ºè®®': rec} for rec in result_data['summary']['recommendations']]
                df_rec = pd.DataFrame(rec_data)
                df_rec.to_excel(writer, sheet_name='æ”¹è¿›å»ºè®®', index=False)
        
        return final_filepath
    
    def save_to_csv(self, result_data: Dict, output_dir: str = "results", 
                   base_filename: str = "bar_results_summary.csv") -> str:
        """ä¿å­˜æ‘˜è¦ç»“æœåˆ°CSVæ–‡ä»¶"""
        self.ensure_results_dir(output_dir)
        filepath = os.path.join(output_dir, base_filename)
        final_filepath = self.get_available_filename(filepath)
        
        # å‡†å¤‡CSVæ•°æ®
        csv_data = []
        for group_name, group_results in result_data['results'].items():
            for result in group_results:
                csv_data.append({
                    'Group': group_name,
                    'Indicator': result['indicator_name'],
                    'Mean': round(result['mean'], 4),
                    'SD': round(result['sd'], 4),
                    'Sample_Size': result['sample_size'],
                    'Error_Bar_Type': result['detected_type'],
                    'Conversion_Method': result['conversion_method'],
                    'Confidence': round(result['confidence'], 3)
                })
        
        # å†™å…¥CSV
        if csv_data:
            df = pd.DataFrame(csv_data)
            df.to_csv(final_filepath, index=False, encoding='utf-8')
        
        return final_filepath


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='å¸¦è¯¯å·®çº¿æŸ±çŠ¶å›¾æ•°æ®è½¬æ¢å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨æ­¥éª¤:
  1. ç”Ÿæˆæ¨¡æ¿: python bar_converter.py --generate-template
  2. å¡«å†™æ•°æ®: ç¼–è¾‘ template.csvï¼Œä¿å­˜ä¸º data.csv
  3. è½¬æ¢æ•°æ®: python bar_converter.py --convert data.csv
  
è¯¯å·®çº¿ç±»å‹æ”¯æŒ:
  SE    - æ ‡å‡†è¯¯å·®
  SD    - æ ‡å‡†å·®
  CI95  - 95%ç½®ä¿¡åŒºé—´
  CI99  - 99%ç½®ä¿¡åŒºé—´
  2SE   - 2å€æ ‡å‡†è¯¯å·®
  
ç¤ºä¾‹:
  python bar_converter.py --generate-template --indicators 6
  python bar_converter.py --convert data.csv --verbose
  python bar_converter.py --convert data.csv --compare-groups --meta-analysis-format
        """
    )
    
    parser.add_argument('--generate-template', action='store_true', help='ç”ŸæˆCSVæ¨¡æ¿')
    parser.add_argument('--indicators', type=int, default=4, help='æ¨¡æ¿ä¸­çš„æŒ‡æ ‡æ•°é‡ (é»˜è®¤: 4)')
    parser.add_argument('--convert', help='è½¬æ¢CSVæ–‡ä»¶')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--json', action='store_true', help='JSONæ ¼å¼è¾“å‡º')
    parser.add_argument('--output-dir', default='results', help='è¾“å‡ºç›®å½• (é»˜è®¤: results)')
    parser.add_argument('--output-name', default='bar_results', help='è¾“å‡ºæ–‡ä»¶åŸºç¡€åç§° (é»˜è®¤: bar_results)')
    parser.add_argument('--no-csv', action='store_true', help='ä¸ç”ŸæˆCSVæ‘˜è¦æ–‡ä»¶')
    
    # ç»„é—´æ¯”è¾ƒåŠŸèƒ½
    parser.add_argument('--compare-groups', action='store_true', help='å¯ç”¨ç»„é—´æ¯”è¾ƒåŠŸèƒ½')
    parser.add_argument('--comparison-type', choices=['all', 'intervention-baseline'], 
                       default='intervention-baseline', help='æ¯”è¾ƒç±»å‹ (é»˜è®¤: intervention-baseline)')
    parser.add_argument('--confidence-level', type=float, default=0.95, 
                       help='ç½®ä¿¡æ°´å¹³ (é»˜è®¤: 0.95)')
    parser.add_argument('--meta-analysis-format', action='store_true', 
                       help='ç”ŸæˆMetaåˆ†ææ ‡å‡†æ ¼å¼æ–‡ä»¶')
    
    args = parser.parse_args()
    
    converter = BarChartConverter()
    
    try:
        if args.generate_template:
            filename = converter.generate_template(args.indicators)
            print(f"âœ“ å·²ç”Ÿæˆæ¨¡æ¿æ–‡ä»¶: {filename}")
            print(f"  æ”¯æŒ {args.indicators} ä¸ªæŒ‡æ ‡")
            print(f"  è¯·å¡«å†™æ•°æ®åä¿å­˜ä¸º data.csv")
            print(f"  æ”¯æŒçš„è¯¯å·®çº¿ç±»å‹: SE, SD, CI95, CI99, 2SE")
            return
        
        if args.convert:
            print("=== å¸¦è¯¯å·®çº¿æŸ±çŠ¶å›¾æ•°æ®è½¬æ¢å·¥å…· ===\n")
            print(f"å¤„ç†æ–‡ä»¶: {args.convert}")
            
            result = converter.convert_bar_data(args.convert, args.verbose)
            
            # æ‰§è¡Œç»„é—´æ¯”è¾ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
            comparison_data = None
            if args.compare_groups:
                print(f"\næ­£åœ¨æ‰§è¡Œç»„é—´æ¯”è¾ƒåˆ†æ...")
                comparison_data = converter.perform_group_comparisons(
                    result, 
                    args.comparison_type, 
                    args.confidence_level
                )
                
                if not args.json:
                    print_comparison_results(comparison_data, args.verbose)
            
            if args.json:
                output_data = result.copy()
                if comparison_data:
                    output_data['comparisons'] = comparison_data
                print(json.dumps(output_data, indent=2, ensure_ascii=False))
            else:
                print_results(result, args.verbose)
            
            # å¼ºåˆ¶ä¿å­˜ç»“æœ
            print(f"\næ­£åœ¨ä¿å­˜ç»“æœ...")
            try:
                # ä¿å­˜åŸºç¡€ç»“æœï¼ˆåŒ…å«æ¯”è¾ƒæ•°æ®ï¼‰
                excel_filename = f"{args.output_name}.xlsx"
                excel_path = converter.save_to_excel(
                    result, 
                    comparison_data, 
                    args.output_dir, 
                    excel_filename
                )
                
                saved_files = {'excel': excel_path}
                
                # ä¿å­˜CSVæ‘˜è¦æ–‡ä»¶
                if not args.no_csv:
                    csv_filename = f"{args.output_name}_summary.csv"
                    csv_path = converter.save_to_csv(result, args.output_dir, csv_filename)
                    saved_files['csv'] = csv_path
                
                # ç”ŸæˆMetaåˆ†ææ ¼å¼æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if args.meta_analysis_format:
                    meta_files = converter.generate_meta_analysis_formats(
                        result, comparison_data, args.output_dir
                    )
                    saved_files.update(meta_files)
                
                print(f"\nâœ“ ç»“æœå·²ä¿å­˜:")
                print(f"  ğŸ“Š è¯¦ç»†ç»“æœ: {saved_files['excel']}")
                if 'csv' in saved_files:
                    print(f"  ğŸ“‹ æ‘˜è¦ç»“æœ: {saved_files['csv']}")
                
                if args.meta_analysis_format:
                    print(f"  ğŸ“ˆ Metaåˆ†ææ ¼å¼:")
                    for format_name, file_path in meta_files.items():
                        print(f"    - {format_name}: {file_path}")
                
                print(f"\næ–‡ä»¶è¯´æ˜:")
                print(f"- Excelæ–‡ä»¶åŒ…å«å®Œæ•´åˆ†æå’Œå¤šä¸ªå·¥ä½œè¡¨")
                if comparison_data:
                    print(f"- åŒ…å«ç»„é—´æ¯”è¾ƒç»“æœå’Œç½®ä¿¡åŒºé—´åˆ†æ")
                if 'csv' in saved_files:
                    print(f"- CSVæ–‡ä»¶ä¸ºç®€åŒ–æ‘˜è¦ï¼Œä¾¿äºå¯¼å…¥å…¶ä»–è½¯ä»¶")
                if args.meta_analysis_format:
                    print(f"- Metaåˆ†ææ ¼å¼æ–‡ä»¶å¯ç›´æ¥å¯¼å…¥RevManã€Rç­‰è½¯ä»¶")
                
            except Exception as save_error:
                print(f"âš ï¸  ä¿å­˜æ–‡ä»¶æ—¶å‡ºç°é—®é¢˜: {save_error}")
                print(f"ç»“æœå·²åœ¨å±å¹•ä¸Šæ˜¾ç¤ºï¼Œè¯·æ‰‹åŠ¨ä¿å­˜")
        
        else:
            parser.print_help()
    
    except Exception as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)


def print_results(result: Dict, verbose: bool = False):
    """æ‰“å°ç»“æœ"""
    analysis = result['analysis']
    results = result['results']
    summary = result['summary']
    
    print("=" * 60)
    print("è¯¯å·®çº¿ç±»å‹åˆ†æ")
    print("=" * 60)
    
    for group_name, group_analysis in analysis.items():
        print(f"\n{group_name}:")
        for indicator in group_analysis['indicators']:
            status = "âœ“ å®Œæ•´" if indicator['data_complete'] else "âŒ ä¸å®Œæ•´"
            print(f"  {indicator['indicator_name']}: {status}")
            print(f"    å£°æ˜ç±»å‹: {indicator['declared_type']}")
            print(f"    æ£€æµ‹ç±»å‹: {indicator['detected_type']} (ç½®ä¿¡åº¦: {indicator['confidence']:.2f})")
        
        print(f"  æ•´ä½“è´¨é‡: {group_analysis['overall_quality']}")
    
    print(f"\nè¯¯å·®çº¿ç±»å‹åˆ†å¸ƒ:")
    for error_type, count in summary['error_type_distribution'].items():
        print(f"  {error_type}: {count}ä¸ª")
    
    print(f"\nè½¬æ¢æˆåŠŸç‡: {summary['conversion_rate']*100:.1f}% ({summary['successful_conversions']}/{summary['total_indicators']})")
    
    if summary['recommendations']:
        print(f"\næ”¹è¿›å»ºè®®:")
        for rec in summary['recommendations']:
            print(f"  â€¢ {rec}")
    
    print("\n" + "=" * 60)
    print("è½¬æ¢ç»“æœ")
    print("=" * 60)
    
    for group_name, group_results in results.items():
        print(f"\n{group_name}:")
        for result in group_results:
            print(f"  {result['indicator_name']}: Mean={result['mean']:.3f}, "
                  f"SD={result['sd']:.3f}")
            if verbose:
                print(f"    æ–¹æ³•: {result['conversion_method']}, "
                      f"ç±»å‹: {result['detected_type']}, ç½®ä¿¡åº¦: {result['confidence']:.2f}")


def print_comparison_results(comparison_data: Dict, verbose: bool = False):
    """æ‰“å°ç»„é—´æ¯”è¾ƒç»“æœ"""
    print("\n" + "=" * 60)
    print("ç»„é—´æ¯”è¾ƒåˆ†æ")
    print("=" * 60)
    
    print(f"\næ¯”è¾ƒç±»å‹: {comparison_data['comparison_type']}")
    print(f"ç½®ä¿¡æ°´å¹³: {comparison_data['confidence_level']*100}%")
    print(f"æ€»æ¯”è¾ƒæ•°: {comparison_data['total_comparisons']}")
    print(f"æ˜¾è‘—æ¯”è¾ƒæ•°: {comparison_data['significant_comparisons']}")
    
    if comparison_data['comparisons']:
        print(f"\nè¯¦ç»†æ¯”è¾ƒç»“æœ:")
        print("-" * 80)
        
        for comp in comparison_data['comparisons']:
            print(f"\nğŸ“Š {comp['group1_name']} vs {comp['group2_name']} ({comp['indicator_name']})")
            print(f"   Î”Mean = {comp['delta_mean']:.4f}")
            print(f"   SD_diff = {comp['sd_diff']:.4f}")
            print(f"   95% CI: [{comp['ci_lower']:.4f}, {comp['ci_upper']:.4f}]")
            print(f"   Cohen's d = {comp['cohens_d']:.4f}")
            print(f"   På€¼ = {comp['p_value']:.4f}")
            
            # æ˜¾è‘—æ€§æ ‡è®°
            if comp['significant']:
                print(f"   âœ“ {comp['interpretation']}")
            else:
                print(f"   â—‹ {comp['interpretation']}")
            
            if verbose:
                print(f"   è¯¦ç»†ä¿¡æ¯:")
                print(f"     - Hedges' g = {comp['hedges_g']:.4f}")
                print(f"     - tç»Ÿè®¡é‡ = {comp['t_statistic']:.4f}")
                print(f"     - è‡ªç”±åº¦ = {comp['degrees_of_freedom']}")
    
    print("\n" + "=" * 60)
    print("æ¯”è¾ƒç»“æœæ‘˜è¦")
    print("=" * 60)
    
    # æŒ‰æ˜¾è‘—æ€§åˆ†ç»„æ˜¾ç¤º
    significant_comps = [c for c in comparison_data['comparisons'] if c['significant']]
    non_significant_comps = [c for c in comparison_data['comparisons'] if not c['significant']]
    
    if significant_comps:
        print(f"\nâœ“ æ˜¾è‘—å·®å¼‚ ({len(significant_comps)}ä¸ª):")
        for comp in significant_comps:
            direction = "â†‘" if comp['delta_mean'] > 0 else "â†“"
            print(f"  {direction} {comp['indicator_name']}: Î”Mean={comp['delta_mean']:.3f} (p={comp['p_value']:.3f})")
    
    if non_significant_comps:
        print(f"\nâ—‹ æ— æ˜¾è‘—å·®å¼‚ ({len(non_significant_comps)}ä¸ª):")
        for comp in non_significant_comps:
            print(f"    {comp['indicator_name']}: Î”Mean={comp['delta_mean']:.3f} (p={comp['p_value']:.3f})")


if __name__ == "__main__":
    main()
